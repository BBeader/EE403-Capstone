{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection and Tracking Tutorial\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    :download:`Download the tutorial code <detection_and_tracking.ipynb>`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this tutorial, we learn how to use a pretrained network for the detection and tracking of cars and pedestrians.\n",
    "At the end of this tutorial, we will have a full pipeline that takes events in input and outputs the position and id of cars and pedestrians in the field of view.\n",
    "\n",
    "The dataflow of the events is the following:\n",
    "\n",
    "  * Read the events from an event-based camera or a RAW/DAT file\n",
    "  * Preprocess the events \n",
    "  * Two parallel branches: one for ML detection and another for the data association:\n",
    "\n",
    "    * Runtime inference to extract the Detection from preprocessed events\n",
    "    * Noise Filtering (STC, Trail...) + Data association\n",
    "    \n",
    "  * Merge all the results and display them\n",
    "\n",
    "This is the pipeline:\n",
    "\n",
    "![DT_Pipeline](DT_Pipeline.png)\n",
    "\n",
    "Let's start by loading the required libraries and some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "# Import of Metavision Machine Learning binding\n",
    "import metavision_sdk_ml\n",
    "import metavision_sdk_cv\n",
    "from metavision_sdk_core import EventBbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metavision_core.utils import get_sample\n",
    "\n",
    "SEQUENCE_FILENAME_RAW = \"driving_sample.raw\"\n",
    "# if the file doesn't exist, it will be downloaded from Prophesee's public sample server \n",
    "get_sample(SEQUENCE_FILENAME_RAW)\n",
    "assert os.path.isfile(SEQUENCE_FILENAME_RAW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Event Producer\n",
    "\n",
    "The event producer generates a stream of event to feed the pipeline.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We use an object :py:class:`metavision_core.event_io.EventsIterator` to produce the stream of events. We choose to process the data 10ms at a time as this gives a good tradeoff between low latency and performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metavision_core.event_io import EventsIterator\n",
    "\n",
    "DELTA_T = 10000  # 10 ms\n",
    "\n",
    "def init_event_producer():\n",
    "    return EventsIterator(SEQUENCE_FILENAME_RAW, start_ts=0, delta_t=DELTA_T, relative_timestamps=False)\n",
    "\n",
    "#initialize an iterator to get the sensor size\n",
    "mv_it = init_event_producer()\n",
    "ev_height, ev_width = mv_it.get_size()\n",
    "\n",
    "print(\"Dimensions:\", ev_width, ev_height)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "      Dimensions: 1280 720"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detection\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next block to build is the detection block. The detection is done using a neural network inference based on [PyTorch](https://pytorch.org/). The provided model is a Torch model saved using [jit.save()](https://pytorch.org/docs/stable/generated/torch.jit.save.html)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "You can use this model directly with Torch, but our :py:class:`metavision_ml.detection_tracking.object_detector.ObjectDetector` class provides additional features, such as result extraction and parsing, thresholding, non-minima suppression, and more.\n",
    "First, we need a pre-trained PyTorch Jit model with a JSON file of hyperparameters.Check our :ref:`pre-trained models page<chapter_ml_pretrained_models>` to find out how to download one of the object detection PyTorch Jit models.Move the models folders (``red_event_cube_05_2020`` and ``red_histogram_05_2020``) in your local folder or update the path in the following code.\n",
    "\n",
    "Now, we need to load the model and define some parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_EVENT_CUBE_MODEL = False\n",
    "\n",
    "if USE_EVENT_CUBE_MODEL:\n",
    "    NN_MODEL_DIRECTORY = os.path.abspath(os.path.join(os.getcwd(), \"red_event_cube_05_2020\"))\n",
    "else:\n",
    "    NN_MODEL_DIRECTORY = os.path.abspath(os.path.join(os.getcwd(), \"red_histogram_05_2020\"))\n",
    "\n",
    "# check whether we can use the GPU or we should fall back on the CPU\n",
    "DEVICE = \"cpu\"  # \"cpu\", \"cuda\" (or \"cuda:0\", \"cuda:1\", etc.)\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "\n",
    "NN_DOWNSCALE_FACTOR = 2 # divide events input height and width by this factor before applying NN, this gives us a good trade-off between accuracy and performance\n",
    "DETECTOR_SCORE_THRESHOLD = 0.4 # ignore all detections below this threshold\n",
    "NMS_IOU_THRESHOLD = 0.4 # apply Non-Maximum Suppression when the intersection over union (IOU) is above this threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's load the model and create an ``ObjectDetector``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metavision_ml.detection_tracking import ObjectDetector\n",
    "\n",
    "network_input_width  = ev_width  // NN_DOWNSCALE_FACTOR\n",
    "network_input_height = ev_height // NN_DOWNSCALE_FACTOR\n",
    "\n",
    "object_detector = ObjectDetector(NN_MODEL_DIRECTORY,\n",
    "                                 events_input_width=ev_width,\n",
    "                                 events_input_height=ev_height,\n",
    "                                 runtime=DEVICE,\n",
    "                                 network_input_width=network_input_width,\n",
    "                                 network_input_height=network_input_height)\n",
    "object_detector.set_detection_threshold(DETECTOR_SCORE_THRESHOLD)\n",
    "object_detector.set_iou_threshold(NMS_IOU_THRESHOLD)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "Now that we have a detector object, we need to setup a function to use it.\n",
    "Note how, in this case, the object detector has its own data preprocessing component, which we can get with the ``get_cd_processor()`` function. For this reason, we do not need to use other preprocessing tools. It is possible to load the model directly using the Torch functions. In that case, it would be required to prepare the data in input using our preprocessing tools, as presented in the tutorial :doc:`Event Preprocessing <../data_processing/event_preprocessing>`. This approach using directly Torch functions is presented in the tutorial :doc:`Reusing a Torchjit Model <using_torchjit_model>`.\n",
    "\n",
    "Let's get the preprocessing component of our object detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdproc = object_detector.get_cd_processor()\n",
    "frame_buffer = cdproc.init_output_tensor()\n",
    "print(\"frame_buffer.shape: \", frame_buffer.shape)\n",
    "if USE_EVENT_CUBE_MODEL:\n",
    "    assert frame_buffer.shape == (10, network_input_height, network_input_width)\n",
    "else:\n",
    "    assert frame_buffer.shape == (2, network_input_height, network_input_width)\n",
    "assert (frame_buffer == 0).all()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "      frame_buffer.shape:  (2, 360, 640)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can define the detection function. At each iteration we process the events to prepare them for the network.This is done with the ``cdproc.process_events()`` function which builds an input tensor incrementally before it is fed to the neural network.``current_frame_start_ts`` is used to keep track of the starting timestamp of the current frame_buffer.\n",
    "\n",
    "The call to the ``object_detector.process()`` is not done at each iteration of 10ms.It is done at a frequency that is defined for each model at training time. This can be obtained with the function ``get_accumulation_time()``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_accumulation_time = object_detector.get_accumulation_time()\n",
    "def generate_detection(ts, ev):\n",
    "    current_frame_start_ts = ((ts-1) // NN_accumulation_time) * NN_accumulation_time\n",
    "    cdproc.process_events(current_frame_start_ts, ev, frame_buffer)\n",
    "\n",
    "    detections = np.empty(0, dtype=EventBbox)\n",
    "    \n",
    "    if ts % NN_accumulation_time == 0:  # call the network only when defined\n",
    "        # call neural network to detect objects \n",
    "        detections = object_detector.process(ts, frame_buffer)\n",
    "        # reset neural network input frame\n",
    "        frame_buffer.fill(0)\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have the building blocks to read the events, preprocess them, and run the detector to obtain the position of cars and pedestrians.\n",
    "The information obtained from the network only refers to the particular batch of events we passed to the detector.If we want to associate an ID to each object we detected and track them over time, we need to use the ``DataAssociation`` class.We will see how to do it in the next sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise Filtering\n",
    "\n",
    "The data association component works better if we first remove all \"noisy\" events.We can do this in many different ways, but in this tutorial, we will use the ``TrailFilterAlgorithm``.This filter works in the following way: for each pixel, we only keep the first event received, all subsequent events received between the start of the event batch and a temporal threshold defined by the user are discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIL_THRESHOLD=10000\n",
    "trail = metavision_sdk_cv.TrailFilterAlgorithm(width=ev_width, height=ev_height, threshold=TRAIL_THRESHOLD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create the building block for filtering the events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter done after the detection\n",
    "ev_filtered_buffer = trail.get_empty_output_buffer()\n",
    "def noise_filter(ev):\n",
    "    # apply trail filter\n",
    "    trail.process_events(ev, ev_filtered_buffer)\n",
    "    return ev_filtered_buffer.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Association\n",
    "Now that we have the correct input for the data association block, we can initialize it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_data_association():\n",
    "    return metavision_sdk_ml.DataAssociation(width=ev_width, height=ev_height, max_iou_inter_track=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have all the building blocks for detection and tracking. What we are missing is a way to visualize the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video Output Generation\n",
    "\n",
    "First, let's enable the visualization (you can set an environment variable if you want to run this procedurally without display) and the output path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DO_DISPLAY = True and os.environ.get(\"DOC_DISPLAY\", 'ON') != \"OFF\" # display the result in a window\n",
    "OUTPUT_VIDEO = \"\"  # output video (disabled if the string is empty. Set a file name to save the video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video generation is done using [scikit-video](http://www.scikit-video.org/stable/), which provides a wrapper around the FFmpeg library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skvideo.io import FFmpegWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can create the initialization function for the visualization building block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_output():\n",
    "    if OUTPUT_VIDEO:\n",
    "        assert OUTPUT_VIDEO.lower().endswith(\".mp4\"), \"Video should be mp4\"\n",
    "\n",
    "    if DO_DISPLAY:\n",
    "        cv2.namedWindow(\"Detection and Tracking\", cv2.WINDOW_NORMAL)\n",
    "    \n",
    "    return FFmpegWriter(OUTPUT_VIDEO) if OUTPUT_VIDEO else None\n",
    "\n",
    "if OUTPUT_VIDEO or DO_DISPLAY:\n",
    "    frame = np.zeros((ev_height, ev_width * 2, 3), dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how we initialized the first frame as an empty Numpy array of the same size as the expected output.\n",
    "\n",
    "Finally, we can create the visualization building block.\n",
    "We use an internal function called `draw_detections_and_tracklets` which takes in input the detections and the tracklets (which are the output of the data association block) and creates a clean visualization.\n",
    "\n",
    "As you can see below, we first create the output with the `draw_detections_and_tracklets` function, and then we optionally display it or store it in a video (or both)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metavision_ml.detection_tracking import draw_detections_and_tracklets\n",
    "from metavision_sdk_core import BaseFrameGenerationAlgorithm\n",
    "\n",
    "def generate_display(ts, ev, detections, tracklets, process_video):\n",
    "    if OUTPUT_VIDEO or DO_DISPLAY:\n",
    "        # build image frame\n",
    "        BaseFrameGenerationAlgorithm.generate_frame(ev, frame[:, :ev_width])\n",
    "        frame[:, ev_width:] = frame[:, :ev_width]\n",
    "        draw_detections_and_tracklets(ts=ts, frame=frame, width=ev_width, height=ev_height,\n",
    "                                         detections=detections, tracklets=tracklets)\n",
    "\n",
    "    if DO_DISPLAY:\n",
    "        # display image on screen\n",
    "        cv2.imshow('Detection and Tracking', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            return False\n",
    "\n",
    "    if OUTPUT_VIDEO:\n",
    "        # write video\n",
    "        process_video.writeFrame(frame[...,::-1].astype(np.uint8))\n",
    "        \n",
    "    return True\n",
    "        \n",
    "def end_display(process_video):\n",
    "    # close video and window\n",
    "    if OUTPUT_VIDEO:\n",
    "        process_video.close()\n",
    "\n",
    "    if DO_DISPLAY:\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Final Pipeline\n",
    "\n",
    "Now that we have all the building blocks, we can instantiate the pipeline and execute it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_it = init_event_producer() # initialize the iterator to read the events\n",
    "object_detector.reset() # reset the object detector internal memory before processing a sequence\n",
    "data_assoc = init_data_association() #  initialize the data association block\n",
    "data_assoc_buffer = data_assoc.get_empty_output_buffer()\n",
    "process_video = init_output() # initialize the video generation\n",
    "\n",
    "END_TS = 10 * 1e6 # process sequence until this timestamp (None to disable)\n",
    "\n",
    "for ev in mv_it:\n",
    "    ts = mv_it.get_current_time()\n",
    "\n",
    "    if END_TS and ts > END_TS:\n",
    "        break\n",
    "    \n",
    "    # run the detectors and get the output\n",
    "    detections = generate_detection(ts, ev)\n",
    "\n",
    "    # remove noisy events for processing with the data association block    \n",
    "    noise_filtered_ev = noise_filter(ev)\n",
    "\n",
    "    # compute tracklets\n",
    "    data_assoc.process_events(ts, noise_filtered_ev, detections, data_assoc_buffer)\n",
    "    tracklets = data_assoc_buffer.numpy()\n",
    "    \n",
    "    if not generate_display(ts, ev, detections, tracklets, process_video):\n",
    "        # if the generation is stopped using `q`, break the loop\n",
    "        break\n",
    "\n",
    "# finalize the recording\n",
    "end_display(process_video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reloading Detections from a Text File\n",
    "\n",
    "In this section, we demonstrate how the same pipeline can be used by gathering detected boxes from an external source. Here, we use the results from a previous run of our pipeline, where we stored the output in a CSV file, but any external detector could be used.\n",
    "\n",
    "The same process could be used with any external object detector or ground truth, as long as the prediction bounding boxes are converted into a numpy array of `metavision_sdk_core.EventBbox.dtype`.\n",
    "\n",
    "First, let's load the detections from a CSV file using the `detections_csv_loader()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "from metavision_ml.detection_tracking import detections_csv_loader\n",
    "\n",
    "offline_detections = \"driving_sample_detections.txt\"\n",
    "\n",
    "# if the file doesn't exist, it will be downloaded from Prophesee's public sample server \n",
    "if not os.path.exists(offline_detections):\n",
    "    urlretrieve(\"https://dataset.prophesee.ai/index.php/s/cupakTZAbfnXrJe/download\", filename=offline_detections)\n",
    "        \n",
    "NN_ACCUMULATION_TIME = 50000\n",
    "\n",
    "dic_ts_eventbbox = detections_csv_loader(offline_detections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now we can create a function that loads the detection iteratively at the correct timestamp:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_detection(ts, ev):\n",
    "    detections = np.empty(0, dtype=EventBbox)\n",
    "    if ts % NN_accumulation_time == 0:\n",
    "        if ts in dic_ts_eventbbox:\n",
    "            detections = dic_ts_eventbbox[ts]\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, here is the new pipeline. This pipeline is the same as the one presented in the previous section, but with the detector replaced with our `load_detection` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_it = init_event_producer() # initialize the iterator to read the events\n",
    "data_assoc = init_data_association() #  initialize the data association block\n",
    "data_assoc_buffer = data_assoc.get_empty_output_buffer()\n",
    "process_video = init_output() # initialize the video generation\n",
    "\n",
    "END_TS = 2 * 1e6 # process sequence until this timestamp (None to disable)\n",
    "\n",
    "for ev in mv_it:\n",
    "    ts = mv_it.get_current_time()\n",
    "    if END_TS and ts > END_TS:\n",
    "        break\n",
    "   \n",
    "    # load the precomputed detections\n",
    "    detections = load_detection(ts, ev)\n",
    "\n",
    "    # remove noisy events for processing with the data association block    \n",
    "    noise_filtered_ev = noise_filter(ev)\n",
    "\n",
    "    # compute tracklets\n",
    "    data_assoc.process_events(ts, noise_filtered_ev, detections, data_assoc_buffer)\n",
    "    tracklets = data_assoc_buffer.numpy()\n",
    "    \n",
    "    if not generate_display(ts, ev, detections, tracklets, process_video):\n",
    "        # if the generation is stopped using `q`, break the loop\n",
    "        break\n",
    "\n",
    "end_display(process_video)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ".. note::\n",
    "    This tutorial was created using `Jupiter Notebooks <https://jupyter.org/>`_\n",
    "\n",
    "    :download:`Download the tutorial code <detection_and_tracking.ipynb>`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event Preprocessing Tutorial\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    :download:`Download the tutorial code <event_preprocessing.ipynb>`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ".. note::\n",
    "    This tutorial is using some features that are available only with our Professional plan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stream of events is easy to be analyzed directly, but difficult to be used in modern deep learning models, which use [tensors](https://en.wikipedia.org/wiki/Tensor) to accelerate the training. In order to benefit from the existing deep learning ecosystem, as well as to take full advantage of GPUs and other hardware accelerators, we encode our raw events into tensors. \n",
    "\n",
    "In this tutorial, you will learn about different preprocessing methods to extract key information out of the raw events, and to convert them to a dense representation that can be used in deep learning. \n",
    "\n",
    "Before we begin, let's first have a look at the tensor representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Representation\n",
    "\n",
    "### Processing Events Sequentially\n",
    "\n",
    "To prepare the events for the training of Neural Network (NN), we need to group them in batches using a number of ``temporal bins``(also named ``time bins`` or ``tbin`` in the documentation of Metavision). \n",
    "\n",
    "**Why do we use a number of time bins?**\n",
    "\n",
    "Firstly if each tensor contains only one time bin (say of a duration of 20 milliseconds), then the Neural Network (NN) would only be able to process one timestamp at a time which is computationally very inefficient if we have to treat a video of several hours. More importantly, if we want to use Recurrent Neural Network (RNN) to deal with sequential data, then the NN architecture itself already requires us to pass a temporal dimension inside. This time dimension is not only necessary but also essential to the training with RNN. \n",
    "\n",
    "Note that when using multiple time bins, you should subtract the absolute timestamps of events by the starting timestamp of each time bin, so that the sequential events are allocated correctly to their corresponding time bins.\n",
    "\n",
    "\n",
    "### Shape of the tensor\n",
    "\n",
    "We typically create tensors with four dimensions $n,c,h,w$:\n",
    "\n",
    "* n: number of time bins. It adds a time dimension to the data, and enables the data to be processed sequentially. It also allows us to apply the same processing several time bins in a row.\n",
    "* c: number of input channels for each pair  (ùë•,ùë¶)  of spatial coordinates in a tensor. As an analogy, we could say that an RGB image has three channels but a gray level image has only one. Representations with a large number of channels are said to be richer and can contain more information at a cost of memory and processing\n",
    "* h: height of the image\n",
    "* w: width of the image\n",
    "\n",
    "All preprocessing functions share some common parameters:\n",
    "\n",
    "\n",
    "```python\n",
    "def preprocessing(xypt, output_array, total_tbins_delta_t, downsampling_factor=0, reset=True):\n",
    "    pass\n",
    "```\n",
    "\n",
    "where: \n",
    "\n",
    "* ``xypt`` (events): an array of events\n",
    "* ``output_array`` (np.ndarray): a pre-allocated numpy array used to store the output data\n",
    "* ``total_tbins_delta_t`` (us): the duration of the time slice\n",
    "* ``downsampling_factor`` (int): a factor that can be used to reduce the resolution of the tensor (e.g. 2 will scale the resolution in half)\n",
    "* ``reset`` (boolean): specifies whether to clear the output array before using it.\n",
    "\n",
    "Note that in Metavision ML, we save the preprocessed events in n-dimensional array or ndarray, which can be easily converted to tensor during training."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We also provide visualization functions to display the preprocessing results. For more information see :py:class:`metavision_ml.preprocessing.viz`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Functions\n",
    "\n",
    "In this section, we will present the preprocessing functions provided in Metavision ML.\n",
    "\n",
    "### Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt  # graphic library, for plots \n",
    "plt.rcParams['figure.figsize'] = [8, 6]\n",
    "\n",
    "from metavision_core.event_io import EventDatReader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Event Data\n",
    "\n",
    "In this tutorial, we will use a spinner sample data.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Use our custom function :py:class:`metavision_core.utils.samples.get_sample` to download Prophesee sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"spinner.dat\"\n",
    "from metavision_core.utils import get_sample\n",
    "\n",
    "get_sample(path, folder=\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the raw events to the variable ``events``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record = EventDatReader(path)\n",
    "height, width = record.get_size()\n",
    "print('record dimensions: ', height, width)\n",
    "start_ts = 1 * 1e6\n",
    "record.seek_time(start_ts)  # seek in the file to 1s\n",
    "\n",
    "delta_t = 50000 #sampling duration\n",
    "events = record.load_delta_t(delta_t)  # load 50 milliseconds worth of events\n",
    "events['t'] -= int(start_ts) # important! almost all preprocessing use relative time!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram\n",
    "\n",
    "Histogram generation is the simplest preprocessing method. It consists in assigning each event to a cell depending on its position (x,y), and to a time bin, depending on its timestamp (t). Then, we count the number of total events in each cell and time bin. We count each polarity separately, one for each output channel. This gives us in total two output channels.\n",
    "\n",
    "Let $H$ be a tensor of four dimensions $n,c,h,w$. \n",
    "\n",
    "\n",
    "For each event $(x,y,p,t)$ we update the histogram of the corresponding time bin accordingly:\n",
    "$$ H(\\left \\lfloor{\\frac{t}{\\Delta}}\\right \\rfloor , p, y, x) = H(\\left \\lfloor{\\frac{t}{\\Delta}}\\right \\rfloor, p, y, x) + 1 $$\n",
    "\n",
    "where:  $\\Delta$ is the time interval in microseconds. \n",
    "      \n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "For more information, see :py:class:`metavision_ml.preprocessing.event_to_tensor.histo`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example, we divide the sampling duration into four time bins, and visualize the result of the ``histogram`` for the 2nd time bin using ``viz_histo``. The intensity of the color increases with the number of events in each histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metavision_ml.preprocessing import histo\n",
    "from metavision_ml.preprocessing.viz import viz_histo\n",
    "\n",
    "tbins=4\n",
    "\n",
    "volume = np.zeros((tbins, 2, height, width), dtype=np.float32)\n",
    "\n",
    "histo(events, volume, delta_t)\n",
    "\n",
    "im = viz_histo(volume[1])\n",
    "\n",
    "plt.imshow(im)\n",
    "plt.tight_layout()\n",
    "plt.title('Histogram', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the ``viz_histo`` function visualizes only the difference of histograms between the positive and the negative channel. Therefore, in the figures above we only see the events near the beginning and the end of the time bin where there are big differences between the two polarities. The histograms of the two polarities cancel each other out in the middle of the time bin. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram Difference\n",
    "If there is a significant amount of activities in one area of the field of view, it might happen that the same histogram cell receives several positive and negative events. If you are interested only in the final event offset (considering that a positive and a negative event on the same cell cancel each other), you can use `diff` processing method.\n",
    "\n",
    "`diff` will compute the difference (positive channel - negative channel) of the two polarities, and thus output one channel.\n",
    "\n",
    "Note that you must ensure that the output array has a _signed_ data type such as `np.int16` or `np.float32` (for more information see the numpy documentation about [data types](https://numpy.org/devdocs/user/basics.types.html)).\n",
    "\n",
    "$$ D(t, p, y, x) = H(t, 1, y,x) - H(t, 0, y,x) ,\\text{  for } x,y,t \\in [0; Width]\\times [0; Height] \\times [0; \\Delta] $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "For more information, see :py:class:`metavision_ml.preprocessing.event_to_tensor.diff`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as before, we can visualize the results using the `viz_diff` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metavision_ml.preprocessing import diff\n",
    "from metavision_ml.preprocessing.viz import viz_diff\n",
    "\n",
    "volume = np.zeros((tbins, 1, height, width))\n",
    "\n",
    "diff(events, volume, delta_t)\n",
    "im = viz_diff(volume[1,0])\n",
    "\n",
    "plt.imshow(im)\n",
    "plt.tight_layout()\n",
    "plt.title('Histogram Difference', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the ``histogram difference`` algorithm shows a more gradual change of the number of events over a time bin. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Surface\n",
    "\n",
    "Time surface is another method of processing events which consists in storing the timestamp of the last received event in each pixel. Polarities are considered separately, so this method outputs two channels.\n",
    "\n",
    "One interesting property of the time surface is that its gradient is an approximation of the normal optical flow. For more information on time surfaces, refer to [Asynchronous frameless event-based optical flow](https://www.neuromorphic-vision.com/public/publications/12/publication.pdf).\n",
    "\n",
    "To properly use time surfaces, you need to implement a resetting or decaying mechanism, which allows to \"forget\" old events over time. This can be done in a linear or exponential way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Time Surface\n",
    "\n",
    "In a linear time surface, the values of the pixel in the time surface are reset every time you compute a new batch. Effectively, each cell of the time surface contains the timestamp of the latest received event minus the timestamp at the beginning of the batch.\n",
    "$$TS(p, y, x) = t - t_0 \\text{ for each event } (x, y, p, t) \\text{ in the event batch }$$\n",
    "\n",
    "where: $t_0$ is the starting timestamp of the event batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metavision_ml.preprocessing import timesurface\n",
    "from metavision_ml.preprocessing.viz import filter_outliers\n",
    "\n",
    "volume = np.zeros((tbins, 2, height, width))\n",
    "\n",
    "timesurface(events, volume, delta_t, normed=True)\n",
    "\n",
    "plt.imshow(filter_outliers(volume[1,0], 7))\n",
    "plt.tight_layout()\n",
    "plt.colorbar()\n",
    "plt.title('Linear Time Surface', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a linear transition of color from blue to yellow, indicating the time trajectory from old to new events. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exponential-Decay Time Surface\n",
    "\n",
    "In an exponential-decay time surface, the timestamps contained in the time surface are \"decayed\" over time in an exponential way. \n",
    "\n",
    "The formula for the exponential decay time surface at time $t_i$ is the following:\n",
    "\n",
    "$$TS_{t_{i}}(p, y, x) = \\text{exp}\\left( -\\frac{t_i-t}{\\tau} \\right) \\text{ for each event } (x, y, p, t)  \\text{ when } t \\leq t_i $$\n",
    "\n",
    "Where: \n",
    "\n",
    "* $t_i$ is the moment when we want to compute the time surface\n",
    "* t is the timestamp of any event before $t_i$\n",
    "* the $\\tau$ constant is used to either increase or decrease the importance of the past events. If $\\tau$ is small, then only recent events can really contribute to the time surface; while with a larger $\\tau$ factor more older events can contribute to the time surface. \n",
    "\n",
    "The main advantage of an exponential decay time surface is that it can contain an arbitrary long time slice of events while still giving more weight to recent events.\n",
    "\n",
    "Let‚Äôs reload more events to see the influence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record.seek_time(3 * 1e6)  # seek in the file to 3s\n",
    "\n",
    "larger_delta_t = 1000000  # 1s\n",
    "timesurface_events = record.load_delta_t(larger_delta_t)  # load 1s of events\n",
    "timesurface_events['t'] -= int(3 * 1e6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first compute a linear time surface and assigned it to the variable ``volume`` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume = np.zeros((1, 2, height, width))\n",
    "timesurface(timesurface_events, volume, larger_delta_t, normed=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we apply the exponential decay function on it, with two different $\\tau$ values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = np.exp(-(delta_t - volume) / 5e4  )\n",
    "v2 = np.exp(-(delta_t - volume) / 5e5  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the Exponential-Decay time surface with regard to the two $\\tau$ values (x axis) and the two polarities (y axis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the result\n",
    "fig, axes_array= plt.subplots(nrows=2, ncols=2)\n",
    "\n",
    "im0 = axes_array[0, 0].imshow(v1[0, 0])\n",
    "fig.colorbar(im0, ax=axes_array[0, 0], shrink=1)\n",
    "\n",
    "im1 = axes_array[1, 0].imshow(v1[0, 1])\n",
    "fig.colorbar(im1, ax=axes_array[1, 0], shrink=1)\n",
    "\n",
    "im2 = axes_array[0, 1].imshow(v2[0, 0])\n",
    "fig.colorbar(im2, ax=axes_array[0,1], shrink=1)\n",
    "\n",
    "im3 = axes_array[1, 1].imshow(v2[0, 1])\n",
    "fig.colorbar(im3, ax=axes_array[1, 1], shrink=1)\n",
    "    \n",
    "axes_array[0, 0].set_ylabel(\"Polarity 0\")\n",
    "axes_array[1, 0].set_ylabel(\"Polarity 1\")\n",
    "axes_array[1, 0].set_xlabel(\"œÑ 0.05s\")\n",
    "axes_array[1, 1].set_xlabel(\"œÑ 0.5s\")\n",
    "plt.tight_layout()\n",
    "fig.subplots_adjust(top=0.88)\n",
    "fig.suptitle('Exponential-Decay Time Surface', fontsize=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figures above illustrate the effect of $\\tau$ coefficient: with a smaller $\\tau$ (on the left), we see a gradual change of color contour which corresponds to the moving direction of the spinner. While with a larger $\\tau$ (on the right) this information is smoothed out, but we get a denser representation of the motion. Depending on the moving speed of the object and the type of application, $tau$ needs to be tuned accordingly.\n",
    "\n",
    "Next, let's have a look at a more advanced preprocessing method: Event Cube."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event Cube\n",
    "\n",
    "Event cube were designed to combine the simplicity of Histogram with the time information of Time Surface. \n",
    "\n",
    "In event cubes, each time bin is split into micro time bins. Similar to histogram generation, each event is assigned to a cell depending on its position (x,y), and to a time bin, depending on its timestamp (t). But instead of counting the events in a binary way (+1 for a new event) like what we did in Histogram, each event is counted by its temporal distance to the center of the neighboring micro time bins. \n",
    "\n",
    "Let $C$ be a tensor of four dimensions $(n, 2 * c, h, w)$. $c$ is the number of micro time bins.\n",
    "\n",
    "For each event $(x,y,p,t)$, we update the linearly-weighted histogram accordingly, for each polarity:\n",
    "\n",
    "$$ EC(t_i , c_i, y, x) = EC(t_i , c_i , y, x) + \\max(0,1- \\left | \\left \\lfloor{t^*}\\right \\rfloor -t^* \\right |)$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* T : the total time interval(us) in which events are taken (us)\n",
    "* $t_0$: the starting timestamp (us)\n",
    "* $\\lfloor{t^*}\\rfloor$: the left and right closest micro time bin no. to $t^*$\n",
    "* $\\Delta ={\\frac{T}{n}}$,   the time interval in each time bin(us)\n",
    "* $t_i  = \\left \\lfloor{\\frac{t}{\\Delta}}\\right \\rfloor$;   the sequential no. of the time bin\n",
    "* $t^* = c \\times \\frac{t_i- t_0 }{\\Delta} -0.5$;   the relative temporal distance to the centre of the corresponding micro time bin\n",
    "* $c_i  = (\\left \\lfloor{\\frac{t\\times c}{\\Delta}}\\right \\rfloor \\mod  c)$;   the sequential no. of the micro time bin\n",
    "\n",
    "\n",
    "To better understand this, let's take a look at two individual positive events below and observe their temporal positions in the micro time bins. Then, we will compare their level of contribution to the micro time bin $n$.\n",
    "\n",
    "![Event contribution to a given tbin in the event volume](event_volume_1d.png \"Schematic of the interpolation kernel\")\n",
    "\n",
    "In the figure above, the first event in blue falls into the micro time bin $n-1$, and it is temporally far from the middle point of the micro time bin $n$, so its contribution to the micro time bin $n$ is small. The second event, in purple, is temporally much closer to the middle of the micro time bin $n$, and therefore has a bigger contribution to the micro time bin $n$.\n",
    "\n",
    "In essence, Event Cube algorithm distribute the value (1) of each event over the two nearest time bins. The weights of the two neighboring time bins are determined by the temporal distance between the event and the centroids of the neighboring time bins. For instance, the first event in blue contributes to the micro time bin $n-1$ and $n$. The second event in purple is too far from $n-1$ to contribute to it, but contributes to $n+1$ and $n$.\n",
    "\n",
    "For comparison, let's see how the same events would contribute to a standard histogram:\n",
    "\n",
    "![Event contribution to a given tbin in the histogram](histo1d.png \"Histogram behaviour for comparison\")\n",
    "\n",
    "Each event contributes only to the time bin where it falls into. All the events that fall into the same time bin give the same contribution to it, that is the value +1. The temporal information of the event within this time bin is therefore not preserved. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "For more information, see :py:class:`metavision_ml.preprocessing.event_to_tensor.event_cube`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now visualize the spinner events with Event Cube preprocessing method.\n",
    "\n",
    "As the previous examples (except for the Exponential-Decay Time Surface where we deliberately chose a bigger time interval), we split the events into two time bins. But here for each time bin, we also split it further into three micro time bins per polarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metavision_ml.preprocessing import event_cube\n",
    "from metavision_ml.preprocessing.viz import viz_event_cube_rgb\n",
    "\n",
    "volume = np.zeros((1, 6, height, width), dtype=np.float32)\n",
    "\n",
    "event_cube(events, volume, delta_t)\n",
    "plt.figure()\n",
    "\n",
    "fig, axes_array= plt.subplots(nrows=2, ncols=3)\n",
    "for i in range(6):\n",
    "    polarity = i % 2\n",
    "    microtbin = i // 2\n",
    "    img = volume[0, i]\n",
    "    img = filter_outliers(img, 2)\n",
    "    img = (img - img.min()) / (img.max() - img.min())\n",
    "    axes_array[polarity, microtbin].imshow(img)\n",
    "\n",
    "axes_array[0, 0].set_ylabel(\"Polarity 0\")\n",
    "axes_array[1, 0].set_ylabel(\"Polarity 1\")\n",
    "axes_array[1, 0].set_xlabel(\"1st ¬µtbin\")\n",
    "axes_array[1, 1].set_xlabel(\"2nd ¬µtbin\")\n",
    "axes_array[1, 2].set_xlabel(\"3rd ¬µtbin\")\n",
    "\n",
    "plt.suptitle('Event Cube with 3 micro time bins \\n per polarity', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's visualize the 3 channels (micro tbins) with our **RGB visualization** function ``viz_event_cube_rgb``, for events with positive polarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = viz_event_cube_rgb(volume[0])\n",
    "plt.figure()\n",
    "plt.imshow(img)\n",
    "plt.title(\"RGB visualization of the micro time bins\", fontsize=20)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Your Own Preprocessing Function\n",
    "\n",
    "If the available preprocessing functions are not suited for your application, you can easily develop your own preprocessing algorithm with the tools provided in Metavison ML.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The first step is to define a function that matches this signature ``preprocessing_function(input_events, output_tensor, **kwargs)``. Then we can register it using the :py:class:`metavision_ml.preprocessing.register_new_preprocessing` function. This will make it available to the other modules of the Metavision ML package.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To learn more about how to register the preprocessed events, refer to the tutorial :ref:`Precomputing features as HDF5 datasets <precomputing_features_hdf5_datasets>`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example, we will implement a function to create a binary frame: a frame where each pixel is set to one or zero depending on whether or not there were events received on that pixel. This is a very naive and simple preprocessing method, which loses all the temporal and frequency information, but it might be sufficient for some applications, especially in a low power setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_frame_preprocessing(xypt, output_array, total_tbins_delta_t, downsampling_factor=0, reset=True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        xypt (events): structured array containing events\n",
    "        output_array: ndarray \n",
    "        total_tbins_delta_t (int): total duration of the time slice \n",
    "        downsampling_factor (int): parameter used to reduce the spatial dimension of the obtained feature.\n",
    "                                   in practice, the original coordinates will be multiplied by 2**(-downsampling_factor).\n",
    "        reset (boolean): whether to reset the output_array to 0 or not.\n",
    "    \"\"\"\n",
    "    num_tbins = output_array.shape[0]\n",
    "    dt = int(total_tbins_delta_t / num_tbins)\n",
    "\n",
    "    if reset:\n",
    "        output_array[...] = 0\n",
    "\n",
    "    ti = 0  # time bins starts at 0\n",
    "    bin_threshold_int =  int(math.ceil(dt))  # integer bound for the time bin \n",
    "    for i in range(xypt.shape[0]):\n",
    "        x, y, p, t = xypt['x'][i] >> downsampling_factor, xypt['y'][i] >> downsampling_factor, xypt['p'][i], xypt['t'][i]  # get the event information, scaled if needed\n",
    "        # we compute the time bin\n",
    "        if t >= bin_threshold_int and ti + 1 < num_tbins:\n",
    "\n",
    "            ti = int(t // dt)\n",
    "            bin_threshold_int = int(math.ceil((ti+1) * dt))\n",
    "        output_array[ti, p, y, x] = 1  # set one for each event we receive\n",
    "\n",
    "from numba import jit\n",
    "# the jit function (Just-in-Time) can be used as a decorator or directly like this\n",
    "numba_binary_frame_preprocessing = jit(binary_frame_preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we use [Numba](http://numba.pydata.org/) to optimize the performance of the created function. This is not mandatory, a normal Python function will work, but using Numba will reduce the cost of this function without any major drawback.\n",
    "\n",
    "Then we can register the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metavision_ml.preprocessing import register_new_preprocessing\n",
    "\n",
    "n_input_channels = 2  # one channel for each polarity\n",
    "\n",
    "register_new_preprocessing(\"binary_frame_preprocessing\", n_input_channels, numba_binary_frame_preprocessing, viz_diff)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "For storing preprocessed features, refer to the tutorial :doc:`Precomputing features as HDF5 datasets <precomputing_features_hdf5_datasets>`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ".. note::\n",
    "    This tutorial was created using `Jupiter Notebooks <https://jupyter.org/>`_\n",
    "\n",
    "    :download:`Download the tutorial code <event_preprocessing.ipynb>`."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

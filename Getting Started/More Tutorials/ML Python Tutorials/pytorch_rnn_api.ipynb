{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch RNN API\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    :download:`Download the tutorial code <pytorch_rnn_api.ipynb>`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this tutorial is using some features that are available only with our Professional plan.\n",
    "\n",
    "Events describe a difference of luminance in the scene, so running neural networks for pattern recognition is not as performant if you do not use some form of memory. This is why we tend to use Recurrent Neural Network to deal with video sequences.\n",
    "\n",
    "In this tutorial, we will explain one of the fundamental model architectures that we have used in our Metavision API: ConvRNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from metavision_ml.core import temporal_modules as tm, modules as m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A typical Sequence\n",
    "\n",
    "In our API, an EB video sequence is represented by 5-d tensors of shape T,B,C,H,W:\n",
    "\n",
    "  - T: number of time bins\n",
    "  - B: batch size\n",
    "  - C: number of channels\n",
    "  - H: height of the input\n",
    "  - W: width of the input\n",
    "\n",
    "In Pytorch, 2D operations usually only take the last 4 dimensions. To ease the transformation between the 5d and 4d tensors, we use function ``time_to_batch`` to \"flatten\" the 5d tensor to 4d, and function ``batch_to_time`` to unflatten the 4d tensor to 5d. These functions can be imported from <metavision_ml.core.temporal_modules>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t,b,c,h,w = 3,4,5,64,64 #random 5-d input\n",
    "x = torch.randn(t,b,c,h,w)\n",
    "\n",
    "print(\"Input shape: \", x.shape)\n",
    "\n",
    "flatten_x, _ = tm.time_to_batch(x)\n",
    "\n",
    "print(\"Flattened shape: \", flatten_x.shape)\n",
    "\n",
    "unflatten_back_x = tm.batch_to_time(flatten_x, b)\n",
    "\n",
    "print(\"Back to original shape: \", unflatten_back_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also created a wrapper around torch nn module, so that all the before-mentioned transformations can be performed within the wrapper called ``seq_wise``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cout = 16\n",
    "layer = m.ConvLayer(c, cout)\n",
    "\n",
    "y = tm.seq_wise(layer)(x)\n",
    "\n",
    "print(\"Output shape: \", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, what this wrapper does is just to flatten the 5d input tensor to 4d before passing to the 2d operator, then convert it back to 5d as result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvRNN\n",
    "\n",
    "All this is great if we just care about processing time steps in parallel in isolation, but what if we want to propagate information sequentially? This is where the ConvRNN comes to the rescue.\n",
    "\n",
    "You can call the ConvRNN class from the <metavision_ml.core.temporal_modules> as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_layer = tm.ConvRNN(c, cout)\n",
    "\n",
    "# let's test it on the input x\n",
    "y = rnn_layer(x)\n",
    "\n",
    "print('output: ', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvRNN layers\n",
    "\n",
    "ConvRNN class is composed of 2 parts:\n",
    "\n",
    "  - a 2d **input-to-hidden** layer performed in parallel (using the \"flattening\" operations described above)\n",
    "  - a for-loop of 2d operations for **hidden-to-hidden** transformations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the input-to-hidden layer outputs 4 times the number of output channels, because for the RNN part we use LSTM unit which contains 4 components:\n",
    "\n",
    "  - input\n",
    "  - cell\n",
    "  - forget\n",
    "  - output\n",
    "\n",
    "See https://en.wikipedia.org/wiki/Long_short-term_memory for detailed information. \n",
    "\n",
    "Let's take a look in the ConvRNN layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    x1 = rnn_layer.conv_x2h(x)\n",
    "    print('first part: ', x1.shape)\n",
    "    x2 = rnn_layer.timepool(x1) \n",
    "    print('second part: ', x2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The RNN Cell\n",
    "\n",
    "The for-loop is a bit hidden in the previous example, let's take a closer look at the RNN component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "x2h = nn.Conv2d(c, cout, kernel_size=3, stride=1, padding=1)\n",
    "h2h = nn.Conv2d(cout, cout, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "x1 = tm.seq_wise(x2h)(x)\n",
    "\n",
    "ht = torch.zeros((b,cout,h,w), dtype=torch.float32) # we initialize a hidden state ourselves\n",
    "\n",
    "all_timesteps = []\n",
    "for t, xt in enumerate(x1.unbind(0)):\n",
    "    ht = torch.sigmoid(xt + h2h(ht)) # basic rnn cell\n",
    "    all_timesteps.append(ht[None])\n",
    "    \n",
    "print('final output: ', ht.shape)\n",
    "\n",
    "# Our dummy loss\n",
    "final_output = torch.cat(all_timesteps)\n",
    "loss = final_output.sum()\n",
    "\n",
    "# We can backward everything using Pytorch's AD\n",
    "loss.backward()\n",
    "\n",
    "print('Beginning of gradient of hidden-to-hidden connexion: ', h2h.weight.grad.view(-1)[:10], ' ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masking RNN's hidden state\n",
    "\n",
    "Here you notice that we had to initialize the hidden state ourselves. However with temporal_modules ConvRNN's implementation, the hidden state is handled internally, and is automatically reset when new recordings arrive.\n",
    "\n",
    "You can find a small illustration below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y = rnn_layer(x)\n",
    "    \n",
    "mask = torch.rand(b) > 0.7 # imagine that 70% of the batch have been replaced with new videos\n",
    "\n",
    "print('mask: ', mask)\n",
    "print('hidden state first pixel before masking: ', rnn_layer.timepool.prev_h[:,0,0,0])\n",
    "rnn_layer.reset(mask[:,None,None,None])\n",
    "print('hidden state first pixel after masking: ', rnn_layer.timepool.prev_h[:,0,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it, you can now train a ConvRNN!\n",
    "\n",
    "A few closing remarks:\n",
    "\n",
    "- ConvRNN is very memory intensive, so use float16 tensors\n",
    "- You can circumvent some of the memory cost by using Pytorch's gradient checkpointing (not yet implemented)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ".. note::\n",
    "    This tutorial was created using `Jupiter Notebooks <https://jupyter.org/>`_\n",
    "\n",
    "    :download:`Download the tutorial code <pytorch_rnn_api.ipynb>`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " :download:`Download the tutorial code <precomputing_features_hdf5_datasets.ipynb>`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precomputing features as HDF5 datasets\n",
    "\n",
    "Note that this tutorial is using some features that are available only with our Professional plan.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Machine learning is an iterative process, especially during the phase of training. However, it might not be necessary to recompute the features each time. Therefore it is convenient to preprocess the data once and store it in a preprocessed format. In Metavision, we chose to use [HDF5](http://docs.h5py.org/en/stable/), a high-performance database format that is versatile and offers interfaces in many languages, including Python.\n",
    "\n",
    "In this tutorial, we will learn how to precompute features from event-based data and save them in HDF5 format, with the goal of reducing storage size and avoiding repeated preprocessing.\n",
    "\n",
    "\n",
    "In `Metavision ML`, we offer two options to process it:\n",
    "\n",
    "- Use `generate_hdf5.py` Python sample script\n",
    "- Use ML module directly\n",
    "\n",
    "\n",
    "## Use  `generate_hdf5.py`\n",
    "\n",
    "The python script `generate_hdf5.py` is available to convert RAW or DAT files into tensors in HDF5 format, with a predefined preprocessing function. To call it, you can simply use the following command:\n",
    "\n",
    "    python3 <path to generate_hdf5.py> <path to the RAW or DAT file> -o <path to output folder> --preprocess <choice of preprocessing function> \n",
    "    \n",
    "\n",
    "To see the full list of parameters of `generate_hdf5.py`:\n",
    "\n",
    "    python3 <path to generate_hdf5.py> -h\n",
    "\n",
    "    \n",
    "You can process more than one file at a time by using wildcards. For example, if the RAW files are in `dataset/train/`, call:\n",
    "\n",
    "    python3 <path to generate_hdf5.py> [...]\n",
    "    \n",
    "use `--max-duration-ms` to set the maximum storage duration in ms. If the raw data is longer than this period, then several HDF5 files will be produced accordingly. Note that if a NPY label file is associated with the RAW or DAT file, it will be processed as well.\n",
    "\n",
    "\n",
    "## Use ML module directly\n",
    "\n",
    "Alternatively, you can create your own script by importing the ML module. \n",
    "\n",
    "Here are the general steps that you can follow. \n",
    "\n",
    "### 1. Load all necessary libraries and input data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from metavision_ml.preprocessing.viz import filter_outliers\n",
    "from metavision_ml.preprocessing.hdf5 import generate_hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"traffic_monitoring.raw\"\n",
    "# if the file doesn't exist, it will be downloaded from Prophesee's public sample server \n",
    "from metavision_core.utils import get_sample\n",
    "\n",
    "get_sample(input_path, folder=\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Run function `generate_hdf5`\n",
    "\n",
    "with the following main parameters:\n",
    "\n",
    "- input (`paths`)\n",
    "- output (`output_folder`)\n",
    "- preprocessing function (`preprocess`)\n",
    "- sampling period (`delta_t`)\n",
    "\n",
    "\n",
    "Note that here we choose to use `timesurface` to process the raw events, but you could use any of the available preprocessing methods or a customized function that has been registered using the `register_new_preprocessing()` function in <metavision_ml/preprocessing> module. Take a look at the preprocessing tutorial for more information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = \".\"\n",
    "output_path = output_folder + os.sep + os.path.basename(input_path).replace('.raw', '.h5')\n",
    "if not os.path.exists(output_path):\n",
    "    generate_hdf5(paths=input_path, output_folder=output_folder, preprocess=\"timesurface\", delta_t=250000, height=None, width=None,\n",
    "              start_ts=0, max_duration=None)\n",
    "\n",
    "print('\\nOriginal file \\\"{}\" is of size: {:.3f}MB'.format(input_path, os.path.getsize(input_path)/1e6))\n",
    "print('\\nResult file \\\"{}\" is of size: {:.3f}MB'.format(output_path, os.path.getsize(output_path)/1e6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how the size of the stored data is reduced compared to the original input. Also, now that the data is preprocessed, we do not need to re-process it again every time when we want to use it. \n",
    "\n",
    "**Note on the RAW file size and HDF5 file size**\n",
    "\n",
    "The size of the RAW file is directly linked to the number of events in the file. This number is a consequence of the motions and illumination changes that occurred in front of the sensor. The HDF5 file, on the other hand, is frame based. With a given preprocessing function and sampling duration, all HDF5 files should normally contain the same number of tensors regardless of the content. \n",
    "\n",
    "As a result, uncompressed RAW files, which contain a really high temporal precision will be quite large on rich motion scene but a lot smaller for simpler scenes and fixed camera setting, while compressed HDF5 files, with their fixed frame rate, will yield huge gains on those rich scenes but less so on simpler scenes. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A closer look at our HDF5 Datasets \n",
    "\n",
    "We store the precomputed event-based tensors in HDF5 [datasets](http://docs.h5py.org/en/stable/high/dataset.html) named \"data\". The metadata and parameters used during preprocessing are stored as associated attributes of \"data\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first have a look at the \"data\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f  = h5py.File(output_path, 'r')  # open the HDF5 file in read mode\n",
    "print(f['data'])  # show the 'data' dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to NumPy arrays, you can get their `shape` and `dtype` attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_shape = f['data'].shape\n",
    "print(hdf5_shape)  \n",
    "print(f['data'].dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's more, you can get the attributes associated to the datasets as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Attributes :\\n\")\n",
    "for key in f['data'].attrs:\n",
    "    print('\\t', key, ' : ', f['data'].attrs[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now use visualize the data stored within this HDF5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, timesurface in enumerate(f['data'][:10]):\n",
    "    \n",
    "    plt.imshow(filter_outliers(timesurface[0], 7)) #filter out some noise\n",
    "    plt.title(\"{:s} feature computed at time {:d} Î¼s\".format(f['data'].attrs['events_to_tensor'],\n",
    "                                                          f['data'].attrs[\"delta_t\"] * i))\n",
    "    plt.pause(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the HDF5 `dataset` variable is similar to a numpy `ndarray` but has some unique features. An important difference is that, if you read from an HDF5 dataset, the data is actually *read from drive* and put to memory as a numpy array. **If you are handling a large dataset, it is recommended not to read the whole file all at once, to avoid saturating the memory.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enriching an HDF5 file\n",
    "\n",
    "HDF5 files are flexible and can be easily edited, which means that you can add other `datasets` than events. For example, you can include the ground truth labels, statistics and so on. Look at the [h5py documentation](http://docs.h5py.org/en/stable/) for more examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add a confidence score label to our existing \"data\" dataset at each delta time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = np.random.rand(hdf5_shape[0])\n",
    "# we first close the file handle\n",
    "f.close()\n",
    "\n",
    "# the _with_ syntax is a cleaner way to open any file in python\n",
    "with h5py.File(output_path, 'r+') as f:  # r+ reading mode allow to edit a file\n",
    "    try:\n",
    "        # We can create a dataset directly from a numpy array in the following fashion:\n",
    "        label_dset = f.create_dataset(\"confidence\", data=label)\n",
    "        # and then add some metadata to this dataset\n",
    "        label_dset.attrs['label_type'] = np.string_(\"random confidence score\")\n",
    "    except ValueError:\n",
    "        print(\"confidence dataset exists already\")\n",
    "    \n",
    "print(\"The file now contains the following keys: \", h5py.File(output_path, 'r').keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it. We have extended an existing HDF5 dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove output file in the end\n",
    "import os\n",
    "os.remove(output_path)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ".. note::\n",
    "    This tutorial was created using `Jupiter Notebooks <https://jupyter.org/>`_\n",
    "\n",
    "    :download:`Download the tutorial code <precomputing_features_hdf5_datasets.ipynb>`."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training UNet self-supervised flow"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    :download:`Download the tutorial code <flow_trainer.ipynb>`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this tutorial is using some features that are available only with our Professional plan.\n",
    "\n",
    "In this tutorial we will train a UNet self-supervised flow model.\n",
    "First, let's import some libraries required for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from metavision_ml.flow import FlowModel\n",
    "from metavision_ml.preprocessing.hdf5 import generate_hdf5\n",
    "from metavision_core.event_io import EventsIterator\n",
    "\n",
    "\n",
    "DO_DISPLAY = True and os.environ.get(\"DOC_DISPLAY\", 'ON') != \"OFF\" # display the result in a window\n",
    "\n",
    "\n",
    "def namedWindow(title, *args):\n",
    "    if DO_DISPLAY:\n",
    "        cv2.namedWindow(title, *args)\n",
    "\n",
    "def imshow(title, img, delay):\n",
    "    if DO_DISPLAY:\n",
    "        cv2.imshow(title, img)\n",
    "        cv2.waitKey(delay)\n",
    "        \n",
    "def destroyWindow(title):\n",
    "    if DO_DISPLAY:\n",
    "        cv2.destroyWindow(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Architecture\n",
    "\n",
    "To train on event-based tensors in shape T,B,C,H,W, we use a U-Net architecture that is fed with events and spits out dense flow sequences of size T,B,2,H,W.\n",
    "\n",
    "Here is an example of U-Net architecture with a Conv-RNN at the bottleneck. This architecture can be used for various ML predictions for event-based vision such as frame and depth reconstruction or flow estimation.\n",
    "\n",
    "![](flow_unet.png)\n",
    "\n",
    "Here let's see how to implement such a network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metavision_ml.core.modules import ConvLayer, PreActBlock\n",
    "from metavision_ml.core.temporal_modules import SequenceWise, ConvRNN\n",
    "from metavision_ml.core.unet_variants import Unet\n",
    "\n",
    "# we use metavision_ml.core.unet_variants to implement the architecture\n",
    "def unet_rnn(n_input_channels, base=16, scales=3, **kwargs):\n",
    "    down_channel_counts = [base * (2**factor) for factor in range(scales)]\n",
    "    up_channel_counts = list(reversed(down_channel_counts))\n",
    "    middle_channel_count = 2 * down_channel_counts[-1]\n",
    "\n",
    "    def encoder(in_channels, out_channels):\n",
    "        return SequenceWise(\n",
    "            ConvLayer(in_channels, out_channels, 3, separable=True, depth_multiplier=4, stride=2,\n",
    "                      norm=\"none\", activation=\"LeakyReLU\"))\n",
    "\n",
    "    def bottleneck(in_channels, out_channels): return ConvRNN(in_channels, out_channels, 3, 1, separable=True)\n",
    "\n",
    "    def decoder(in_channels, out_channels): return SequenceWise(ConvLayer(\n",
    "        in_channels, out_channels, 3, separable=True, stride=1, activation=\"LeakyReLU\"))\n",
    "\n",
    "    return Unet(encoder, bottleneck, decoder, n_input_channels=n_input_channels, up_channel_counts=up_channel_counts,\n",
    "                         down_channel_counts=down_channel_counts, middle_channel_count=middle_channel_count)\n",
    "\n",
    "\n",
    "flow_model = unet_rnn(5, 2)\n",
    "input_example = torch.randn(3,4,5,240,320)\n",
    "\n",
    "output_example = flow_model(input_example)\n",
    "\n",
    "for i, feature_map in enumerate(output_example):\n",
    "    print('\\ndecoder feature_map#'+str(i)+': '+str(feature_map.shape))\n",
    "\n",
    "result = output_example[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warping functions\n",
    "\n",
    "Dense optical flow computes the observed motion on each pixel in the image plane. In other words, it computes the motion of pixels between a time _t_ and a time _t+1_. This allows us to compute a warping operator $W$ to transform data at time _t_ into data at time _t+1_.\n",
    "\n",
    "We use internally the [grid_sample](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.grid_sample) function from pytorch to Warp an image with the flow, so the flow unit is a displacement in normalized pixels as defined in the grid_sample docummentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [12, 12]\n",
    "\n",
    "from metavision_ml.core.warp_modules import Warping\n",
    "from metavision_ml.flow.viz import draw_arrows\n",
    "\n",
    "to_torch_batch = lambda x: torch.from_numpy(x).permute(2,0,1).contiguous()[None]\n",
    "to_numpy = lambda x: x.data.permute(0,2,3,1).contiguous().numpy()\n",
    "\n",
    "# load an example image\n",
    "path = 'image_example.jpeg'\n",
    "img = cv2.imread(path)\n",
    "assert img is not None, f'{path} does not exist, or is not a valid image'\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "img_th = to_torch_batch(img).float()\n",
    "height, width = img_th.shape[-2:]\n",
    "\n",
    "# contruct the Warping object\n",
    "warper = Warping(height, width)\n",
    "\n",
    "# We start from a grid of all relative pixel coordinates (between -1 and 1)\n",
    "grid_h, grid_w = torch.meshgrid([torch.linspace(-1., 1., height), torch.linspace(-1., 1., width)])\n",
    "grid_xy = torch.cat((grid_w[None,:,:,None], grid_h[None,:,:,None]), dim=3)\n",
    "\n",
    "# generate flow using a random homography\n",
    "rand_mat = torch.randn(2,2)*0.3 + torch.eye(2)\n",
    "grid_diff = grid_xy.view(-1,2).mm(rand_mat).view(1,height,width,2) - grid_xy\n",
    "flow_th = grid_diff.permute(0,3,1,2).contiguous()\n",
    "\n",
    "# display the flow\n",
    "flow = flow_th[0].contiguous().numpy()\n",
    "flow_viz = draw_arrows(img, flow, step=16, threshold_px=0, convert_img_to_gray=True)\n",
    "\n",
    "# use the flow to Warp an image\n",
    "warped = warper(img_th, flow_th)\n",
    "\n",
    "fig, axes_array= plt.subplots(nrows=1, ncols=3, figsize=(12,8), dpi=300)\n",
    "\n",
    "axes_array[0].imshow(to_numpy(img_th)[0].astype(np.uint8))\n",
    "axes_array[0].set_title(\"Input Image\")\n",
    "axes_array[1].imshow(to_numpy(warped)[0].astype(np.uint8))\n",
    "axes_array[1].set_title(\"Warped Image\")\n",
    "axes_array[2].imshow(flow_viz)\n",
    "axes_array[2].set_title(\"Visualization of the flow as arrows\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sharpening\n",
    "\n",
    "In event-based, a Tensor representation using a large `delta_t` will exhibit _motion blur_ just like an Image with a similar exposure time would. However if you can estimate the optical flow during this timespan. You can use it to warp events dutring this timespan so as to compensate for motion. The result should be a sharper reprensentation. We demonstrate here this principle with simulated events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's simulate events from homographies and record the flow between 2 timesteps\n",
    "from metavision_ml.simulator.simulator import EventSimulator\n",
    "from metavision_core.event_io.event_bufferizer import FixedTimeBuffer, FixedCountBuffer\n",
    "from metavision_ml.preprocessing import CDProcessor\n",
    "from metavision_ml.preprocessing.viz import viz_diff\n",
    "from metavision_ml.data.image_planar_motion_stream import PlanarMotionStream\n",
    "\n",
    "read_count_ratio = 0.05\n",
    "read_count = int(read_count_ratio * height * width)\n",
    "C = 0.2\n",
    "refractory_period = 100\n",
    "cutoff_hz = 0\n",
    "\n",
    "\n",
    "image_stream = PlanarMotionStream(path, height, width)\n",
    "fixed_buffer = FixedCountBuffer(read_count)\n",
    "tensorizer = CDProcessor(height, width, 1, \"diff\")\n",
    "simu = EventSimulator(height, width, C, C, refractory_period)\n",
    "\n",
    "ev_tensor_list = []\n",
    "times = []\n",
    "\n",
    "for img, ts in image_stream:\n",
    "    total = simu.image_callback(img, ts)\n",
    "    if total < read_count:\n",
    "        continue\n",
    "    events = simu.get_events()\n",
    "    simu.flush_events()\n",
    "    events = fixed_buffer(events)\n",
    "    if not len(events):\n",
    "        continue\n",
    "    dt = events['t'][-1] - events['t'][0]\n",
    "    events['t'] -= events['t'][0]\n",
    "    ev_count = tensorizer(events).copy()\n",
    "    ev_tensor_list.append(ev_count)\n",
    "    times.append(image_stream.cam.time-1)\n",
    "    if len(ev_tensor_list) == 5:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the sequence\n",
    "plt.rcParams['figure.figsize'] = [16, 12]\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "ev_tensor = np.concatenate(ev_tensor_list, axis=0)\n",
    "\n",
    "fig, axes_array= plt.subplots(nrows=1, ncols=len(ev_tensor))\n",
    "for i in range(len(ev_tensor)):\n",
    "    axes_array[i].set_title('time_bin'+str(i))\n",
    "    axes_array[i].imshow(ev_tensor[i,0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the blurry input\n",
    "blurry = ev_tensor.sum(axis=0)[0]/5\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1)\n",
    "\n",
    "ax.set_title('blurry')\n",
    "ax.imshow(blurry, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metavision_ml.core.warp_modules import sharpen\n",
    "\n",
    "# Let's sharpen the sequence using the flow\n",
    "rvec1, tvec1 = image_stream.cam.rvecs[0], image_stream.cam.tvecs[0]\n",
    "rvec2, tvec2 = image_stream.cam.rvec2, image_stream.cam.tvec2\n",
    "flow = image_stream.cam.get_flow(rvec1, tvec1, rvec2, tvec2, height, width, infinite=True).astype(np.float32)\n",
    "flow_th = torch.from_numpy(flow).permute(2,0,1)[None]\n",
    "ev_tensor_th = torch.from_numpy(ev_tensor)[:,None]\n",
    "\n",
    "# Notice that we divide the flow by the number of tbins \n",
    "# because sharpen expects flow unit to be in normalized displacement/ tbin\n",
    "warped = sharpen(ev_tensor_th, flow_th/5, 0, mode='bilinear')\n",
    "less_blurry = warped.numpy().sum(axis=0)[0,0]/5\n",
    "\n",
    "\n",
    "#Note: here each slice was obtained with irregular sampling, you can obtain a better result if you take \n",
    "#into account the ratio delta_t_bin_j/ total_tbin_delta_t\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1)\n",
    "ax.set_title('sharpen')\n",
    "ax.imshow(less_blurry, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions\n",
    "\n",
    "Dense optical flow computes the observed motion on each pixel in the image plane. In other words, it computes the motion of pixels between a time _t_ and a time _t+1_. This allows us to compute a warping operator $W$ to transform data at time _t_ into data at time _t+1_.\n",
    "\n",
    "A critical characteristics of this operator, is that it is differentiable, and therefore it can be used as a loss function for training:\n",
    "$$\\|W(T_t, F) - T_{t+1}\\|$$\n",
    "where $W$ is a differentiable warping operator, $T_t$ an input tensor at time step $t$ and $F$ the flow at time $t$.\n",
    "\n",
    "However, there are several ways to theoretically warp $T_t$ into $T_{t+1}$. Therefore, to help the network converge to the more plausible vector field for the correct optical flow, we need to add regularizations. Regularization can be done in different ways, such as constraints on the flow smoothness or absolute values, temporal consistency and forward backward consistency. Look to the code in `metavision_ml/flow/losses.py` for more details.\n",
    "\n",
    "The final loss function is a combination of: task-specific loss functions and regularization loss functions.\n",
    "\n",
    "**Task-specific loss functions**\n",
    "These losses are different formulation of the task that the flow is supposed to fulfill: predicting the motion of objects and \"deblurring\" moving edges.\n",
    " * _data term_ this loss ensures that applying the flow to warp a tensor at time $t$ will match the tensor at time $t+1$\n",
    " * _time consistency loss_ this loss checks the flow computed at timestamp $t_i$ is also correct at time $t_{i+1}$ as most motions are consistent over time. This assumption doesn't hold for fast moving objects.\n",
    " * _backward deblurring loss_ this loss is applied backwards to avoid the degenerate solution of a flow warping all tensors into one single point or away from the frame (such a flow would have a really high loss when applied backward). We call this \"deblurring loss\" as it allows us to warp several time channels to a single point and obtain an image that is sharper (lower variance).\n",
    " \n",
    "**Regularization loss functions**\n",
    " * _smoothness term_ this loss is a first order derivative kernel applied to the flow to minimise extreme variations of flow. \n",
    " * _second order smoothness term_ this loss is a second order derivative kernel encouraging flows to be locally colinear.\n",
    " * _L1 term_ this term also penalises extreme values of flow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the training\n",
    "\n",
    "You can run the training using the `train_flow.py` sample\n",
    "\n",
    "    python3 train_flow.py <path to the output directory> <path to the input directory>\n",
    "    \n",
    "Note that the dataset directory needs to contain the HDF5 files generated above in a folder structure that contains the `train`, `test`, and `val` directories.\n",
    "\n",
    "The training will generate checkpoint models in the output directory, alongside periodic demo videos useful to visualize the partial results.\n",
    "\n",
    "You can also monitor the training using [TensorBoard](https://www.tensorflow.org/tensorboard).\n",
    "\n",
    "An example of a precomputed dataset can be downloaded [from our server](https://dataset.prophesee.ai/index.php/s/V2xcznO24FNBxRs)\n",
    "\n",
    "If you want to run it in this notebook directly, download some dataset and run the following code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from metavision_ml.flow.lightning_model import FlowModel, FlowDataModule\n",
    "from metavision_ml.utils.main_tools import infer_preprocessing\n",
    " \n",
    "# Path to your HDF5 dataset\n",
    "dataset_path = \"<path to your dataset>\"\n",
    "\n",
    "params = argparse.Namespace(\n",
    "        output_dir = \"train_flow_test_drive\",     \n",
    "        dataset_path = dataset_path,   \n",
    "        fast_dev_run = False, \n",
    "        lr = 1e-5, \n",
    "        batch_size = 4, \n",
    "        accumulate_grad_batches = 4, \n",
    "        height_width=None,\n",
    "        demo_every=2, \n",
    "        save_every=2,\n",
    "        max_epochs=10,\n",
    "        precision=16,\n",
    "        cpu=False,\n",
    "        num_tbins=4,\n",
    "        num_workers=1,\n",
    "        no_data_aug=False,\n",
    "        feature_extractor='eminet',\n",
    "        depth=1,\n",
    "        feature_base=16)\n",
    "\n",
    "params.data_aug = not params.no_data_aug\n",
    "\n",
    "array_dim, preprocess, delta_t = infer_preprocessing(params)\n",
    "\n",
    "flow_model = FlowModel(delta_t=delta_t, \n",
    "                       preprocess=preprocess, \n",
    "                       batch_size=params.batch_size,\n",
    "                       num_workers=params.num_workers, \n",
    "                       learning_rate=params.lr, \n",
    "                       array_dim=array_dim,\n",
    "                       loss_weights={\"data\": 1, \n",
    "                                     \"smoothness\": 0.4, \n",
    "                                     \"smoothness2\": 4 * 0.15,\n",
    "                                     \"l1\": 2 * 0.15, \n",
    "                                     \"time_consistency\": 1, \n",
    "                                     \"bw_deblur\": 1},\n",
    "                       feature_extractor_name=params.feature_extractor, \n",
    "                       data_aug=params.data_aug,\n",
    "                       network_kwargs={\"base\": params.feature_base, \n",
    "                                       \"depth\": params.depth, \n",
    "                                       \"separable\": True})\n",
    "\n",
    "    \n",
    "flow_data = FlowDataModule(flow_model.hparams, data_dir=dataset_path,\n",
    "                           test_data_dir=params.val_dataset_path)\n",
    "                           \n",
    "                           \n",
    "# if you want to visualize the Dataset you can run it like this:\n",
    "\n",
    "flow_data.setup()\n",
    "namedWindow(\"dataset\")\n",
    "for im in flow_data.train_dataloader().show():\n",
    "    imshow('dataset', im[...,::-1], 5)\n",
    "destroyWindow(\"dataset\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the flow inference\n",
    "Once your model is trained, you can apply it on any sequence of your choice. We run it here with a pretrained model included in Metavision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from metavision_core.utils import get_sample\n",
    "\n",
    "path = \"hand_spinner.raw\"\n",
    "get_sample(path, folder=\".\")\n",
    "\n",
    "rel_path = \"../\"*6+\"sdk/modules/ml/python_extended\"\n",
    "# lets display the parameters of this script.\n",
    "!python3 $rel_path/samples/flow_inference/flow_inference.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this model was trained with a fixed delta_t we can tune this value for fast object. Increasing the number of micro tbins in your representation can also help dealing with fast objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 $rel_path/samples/flow_inference/flow_inference.py $rel_path/models/flow_model_alpha.ckpt $path --delta-t 18000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reusing a Torchjit Model in Python\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    :download:`Download the tutorial code <using_torchjit_model.ipynb>`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ".. note::\n",
    "    This tutorial is using some features that are available only with our Professional plan."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In the :doc:`detection and tracking tutorial <detection_and_tracking>`, we have seen how we can use the Metavision ML module for detection and tracking.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model we provide is a [torchjit](https://pytorch.org/docs/stable/jit.html), which means that you can also directly access the model using the Torch library.\n",
    "\n",
    "In this tutorial, we will learn how to use our pretrained model with Torch and how to extract the features generated from the model.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "First, we need a pre-trained PyTorch Jit model with a JSON file of hyperparameters. Check our :ref:`pre-trained models page <chapter_ml_pretrained_models>` to find out how to download the object detection PyTorch Jit models. Move the folder `red_histogram_05_2020` to your local directory or update the path in the code that follows.\n",
    "\n",
    "Now let's load the required libraries and the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "import torch\n",
    "\n",
    "from metavision_ml.data import CDProcessorIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jit_path = os.path.join(os.getcwd(), \"red_histogram_05_2020/model.ptjit\")\n",
    "\n",
    "model = torch.jit.load(jit_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loaded model is a [torchjit](https://pytorch.org/docs/stable/jit.html) model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now that we loaded the model, we can reuse its features. They can be used for different purposes, such as training a linear model based on them. In this tutorial, we will use them for visualization. \n",
    "\n",
    "Our detection model expects as input a histogram of events, which we can generate with our :ref:`preprocessing tools <chapter_ml_data_processing>`.\n",
    "\n",
    "Let's load the data using an iterator and create a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"driving_sample.raw\"\n",
    "# if the file doesn't exist, it will be downloaded from Prophesee's public sample server \n",
    "from metavision_core.utils import get_sample\n",
    "\n",
    "get_sample(input_path, folder=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_t = 50000\n",
    "# The processor iterator combines the events iterator with the preprocessing functions\n",
    "proc_iterator = CDProcessorIterator(input_path, \"histo\", delta_t=delta_t, num_tbins=1, preprocess_kwargs={\"max_incr_per_pixel\": 5},\n",
    "                                    device=torch.device('cpu'), height=None, width=None)\n",
    "\n",
    "input_tensor =  next(iter(proc_iterator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now extract the feature map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_maps = model.feature_extractor(input_tensor[None, ...])\n",
    "for feature_map in feature_maps:\n",
    "    print(feature_map.shape)\n",
    "feature_maps = [feature_map.detach().numpy() for feature_map in feature_maps]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our detection network produces features maps at different resolution. Each different feature map corresponds to one channel in the final convolutional layer of the _feature extractor_ network.\n",
    "These feature maps are the features that our network \"considers\" the best for the detection task. To make a comparison, a human looking for cars might search for headlights or wheels, our network uses these feature maps.\n",
    "\n",
    "We can now visualize some of these feature maps: negative values (features that suggest that the object is not a car) are in blue, positives values (features that suggest that the object is a car) in red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(array):\n",
    "    \"\"\"remove outlier values for better visualization\"\"\"\n",
    "    filtered_array = array.copy()\n",
    "    absolute_value = np.abs(filtered_array)\n",
    "    mean = absolute_value.mean()\n",
    "    std = absolute_value.std()\n",
    "    filtered_array[absolute_value > mean + 3 * std] = 0\n",
    "    return filtered_array\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [6, 4]\n",
    "# as a reminder, we first visualize the input of the neural network \n",
    "plt.imshow(proc_iterator.show(time_bin=0))\n",
    "plt.title(\"Neural network input histogram\")\n",
    "plt.show()\n",
    "\n",
    "for index, feature_map in enumerate(feature_maps[0][0, :14]):\n",
    "    feature_map = remove_outliers(feature_map)\n",
    "    plt.imshow(feature_map, cmap=\"coolwarm\")\n",
    "    plt.title(\"feature map number {}\".format(index))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the first 14 features, feature map number 0 looks interesting, as it seems to have a positive correlation with the car in this example. Let's take a closer look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "\n",
    "for input_tensor in islice(proc_iterator,4):\n",
    "    \n",
    "    # we extract this particular feature map\n",
    "    single_feature_map = model.feature_extractor(input_tensor[None, ...])[0][0, 0].detach()\n",
    "    \n",
    "    # Let's display the features alongside the input events.\n",
    "    _, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    feature_map = remove_outliers(single_feature_map.numpy())\n",
    "    ax1.imshow(feature_map, cmap='coolwarm')\n",
    "    ax2.imshow(proc_iterator.show(time_bin=0))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ".. note::\n",
    "    This tutorial was created using `Jupiter Notebooks <https://jupyter.org/>`_\n",
    "\n",
    "    :download:`Download the tutorial code <using_torchjit_model.ipynb>`."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

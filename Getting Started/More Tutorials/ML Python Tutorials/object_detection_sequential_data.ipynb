{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Object Detection for Sequential Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    :download:`Download the tutorial code <object_detection_sequential_data.ipynb>`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ".. note::\n",
    "    This tutorial is using some features that are available only with our Professional plan."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "In this tutorial we will illustrate the training pipeline that we have used in the :doc:`detection and tracking tutorial <../inference/detection_and_tracking>`.\n",
    "The Moving-Mnist Dataset will be used for illustration purpose.\n",
    "\n",
    "First, let's import some libraries required for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "\n",
    "# For Display\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [11, 11]\n",
    "\n",
    "# For training\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TestTubeLogger\n",
    "\n",
    "from metavision_ml.detection.lightning_model import LightningDetectionModel "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy-Problem Dataset\n",
    "\n",
    "For our first training, we will use \"Moving-Mnist\" Dataset. This is an easy dataset that can act as a \"sanity-check\" of the training pipeline.",
    "The dataset delivers video-clips of moving digits with their corresponding boxes."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "**Create a dataloader** \n",
    "\n",
    "You can create a dataloader with the ``make_moving_mnist`` function in the class :py:class:`metavision_ml.data.moving_mnist`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize Moving-Mnist data\n",
    "from metavision_ml.data import box_processing as box_api\n",
    "from metavision_ml.data.moving_mnist import make_moving_mnist\n",
    "\n",
    "\n",
    "# input parameters\n",
    "batch_size = 4 \n",
    "height, width = 128, 128\n",
    "tbins = 20\n",
    "dataloader = make_moving_mnist(train=True, height=height, width=width, tbins=tbins, num_workers=1, batch_size=batch_size, max_frames_per_video=80, max_frames_per_epoch=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = ['background'] + [str(i) for i in range(10)]\n",
    "print('Our Label Map: ', label_map) #We see the classes of MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Batch the recording streams in one grid cell**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 2 ** ((batch_size.bit_length() - 1) // 2) # distribute all batches evenly over the grid\n",
    "ncols = batch_size // nrows\n",
    "grid = np.zeros((nrows, ncols, height, width, 3), dtype=np.uint8) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load 6 batches to plot the digits along the time sequence (tbins)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "from metavision_ml.detection_tracking.display_frame import draw_box_events\n",
    "\n",
    "num_batches = 6\n",
    "show_every_n_tbins = 5\n",
    "\n",
    "fig, axes_array= plt.subplots(nrows=num_batches, ncols=tbins//show_every_n_tbins)\n",
    "\n",
    "for i, data in enumerate(islice(dataloader, num_batches)):\n",
    "    batch, targets = data['inputs'], data['labels']\n",
    "    print('\\n'+'batch nr.{} of shape:{}'.format(i, batch.shape))\n",
    "    print('set mask on/off: {}'.format(data['mask_keep_memory'].view(-1).tolist()))\n",
    "    for t in range(len(batch)):\n",
    "        grid[...] = 0\n",
    "        for n in range(batch_size):\n",
    "            img = (batch[t,n].permute(1, 2, 0).cpu().numpy()*255).copy()\n",
    "            boxes = targets[t][n]\n",
    "            boxes = box_api.box_vectors_to_bboxes(boxes=boxes[:,:4], labels=boxes[:,4]) #convert normal bbox to our EventBbox\n",
    "            img = draw_box_events(img, boxes, label_map, thickness=3, draw_score=False)\n",
    "            grid[n//ncols, n%ncols] = img\n",
    "            im = grid.swapaxes(1, 2).reshape(nrows * height, ncols * width, 3)\n",
    "            \n",
    "        if t%show_every_n_tbins == 0:\n",
    "            axes_array[i, t//show_every_n_tbins].imshow(im.astype(np.uint8))\n",
    "            axes_array[i, t//show_every_n_tbins].set_xlabel(\"t=\"+str(t), fontsize=18)\n",
    "            axes_array[i, t//show_every_n_tbins].set_ylabel(\"batch no.\\n\" + str(i), fontsize=16)\n",
    "        \n",
    "\n",
    "plt.suptitle(\"Moving-MNIST at every 5 time bins\", fontsize=20)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ".. note::\n",
    "    1. the mask is \"ON\" every 4 batches, because the \"max_frames_per_video\" we defined is four times the number of time bins. This mask is used by the RNN API to reset the hidden state.\n",
    "    2. we grouped all the recordings of one batch into one grid cell and plot them over the time sequence (time bins). Since it is difficult to see the difference between neighboring time bins, we only plot the movement every 5 time bins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-Shot Detector with RNN \n",
    "\n",
    "As we are doing object detection on sequential event streams, the conventional detection algorithm needs to be adapted accordingly. In Metavision, the object detection model is trained with a Single-Shot Detector (SSD) with RNN. The model architecture is illustrated in the figure below:\n",
    "\n",
    "![SSD_architecture](SSD_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our SSD model is composed of three main parts:\n",
    "\n",
    "- Feature extractor, containing a base network (indicated in blue) and a multi-scale feature extractor (indicated in green)\n",
    "- 2-Head Predictor (indicated in yellow)\n",
    "- Non-maximum Suppression [NMS] (contained in the last black box)\n",
    "\n",
    "\n",
    "The event-based data stream, after being processed into tensors, is passed first through the base Network to extract basic features. Then a series of multi-scale ConvRNN filters are added to extract features at multiple scales. The extracted features are then concatenated and passed to 2-head predictor to detect objects at multiple scales.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ".. note::\n",
    "    1. Unlike conventional illustration of RNN model, the temporal dimension here is shown on the vertical axis, so this RNN is unrolled vertically. \n",
    "    2. The input tensors are of shape (T,B,C,H,W). For more details on how to create tensors from events, see :ref:`our preprocessing tools tutorials <chapter_ml_data_processing>`. ",
    "       For everything related to the RNN layers, see :doc:`our Pytorch RNN API tutorial <pytorch_rnn_api>`. \n",
    "    3. Each component of the SSD model is self-contained, so you can easily set up a custom SSD model by using a different feature extractor or predictor. \n",
    "\n",
    "Now, let's dive into each key component of the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extractor\n",
    "\n",
    "Let's pass a randomly generated tensor ``x`` to our ``Vanilla`` feature extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metavision_ml.detection.feature_extractors import Vanilla\n",
    "\n",
    "T,B,C,H,W = 10,3,3,height,width #num_tbins, batch_size, no. input features, height, width\n",
    "x = torch.randn(T,B,C,H,W)\n",
    "\n",
    "# get the feature extractor\n",
    "rnn = Vanilla(C, base=64)\n",
    "              \n",
    "features = rnn(x)\n",
    "\n",
    "# Notice That the output is flattened (time sequence is hidden in the batch dimension)\n",
    "for i, feat in enumerate(features):\n",
    "    print('feature#'+str(i)+\": \", feat.shape) #(TxB,C,H,W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result above shows a list of extracted features. They decrease in size progressively to allow a better detection across different object sizes. In this example, the ``Vanilla`` feature extractor conveys 5 spatial levels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anchor Boxes\n",
    "\n",
    "Anchor Boxes are not mentioned in the three components mentioned above. But they are a key feature in SSD. To help detect objects of various shapes, we set a grid of canonical rectangles called \"anchor boxes\" tiled on the feature maps. Since the feature maps are extracted from different scales, the anchors are also of different sizes.\n",
    "\n",
    "\n",
    "Let's visualize the default Anchor boxes provided in Metavision:\n",
    "\n",
    "**1. Initialize the ``Anchors`` class.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metavision_ml.detection.anchors import Anchors\n",
    "from metavision_ml.detection.rpn import BoxHead\n",
    "from metavision_ml.detection.box import xywh2xyxy\n",
    "\n",
    "num_classes = 10\n",
    "label_offset = 1 # classes are shifted because the first logit represents background\n",
    "act = \"softmax\"\n",
    "nlayers = 0\n",
    "step = 60   # stepsize to sample from all the predicted bboxes\n",
    "fill = 127\n",
    "\n",
    "box_coder = Anchors(num_levels=rnn.levels,\n",
    "                   anchor_list=\"PSEE_ANCHORS\",\n",
    "                   variances=[0.1, 0.2] if act == 'softmax' else [1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Sampling from the total anchor boxes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the anchor prediction space\n",
    "anchors_cxywh = box_coder(features, x)\n",
    "anchors = xywh2xyxy(anchors_cxywh)\n",
    "\n",
    "colors = [np.random.randint(0, 255, (3,)).tolist() for i in range(box_coder.num_anchors)]\n",
    "img = np.zeros((H, W, 3), dtype=np.uint8)\n",
    "img[...] = fill\n",
    "\n",
    "anchors = anchors.cpu().data.numpy()\n",
    "anchors = anchors.reshape(len(anchors) // box_coder.num_anchors, box_coder.num_anchors, 4)\n",
    "anchors = anchors[7::step] # sampling from the total bboxes\n",
    "anchors = anchors.reshape(-1, 4)\n",
    "print(anchors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Plot the sampled anchor boxes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axes_array= plt.subplots(nrows=len(anchors)//(box_coder.num_anchors), ncols=box_coder.num_anchors, sharex=True, sharey=True)\n",
    "\n",
    "for i in range(len(anchors)):\n",
    "    anchor = anchors[i] # get the coordinates of bbox\n",
    "    pt1, pt2 = (anchor[0], anchor[1]), (anchor[2], anchor[3])\n",
    "    \n",
    "    ix, iy = i%box_coder.num_anchors, i//box_coder.num_anchors \n",
    "    \n",
    "    img[...] = fill\n",
    "    \n",
    "    cv2.rectangle(img, pt1, pt2, colors[i % box_coder.num_anchors], 2)\n",
    "    \n",
    "    ptcxy = int((anchor[0] + anchor[2])/2), int((anchor[1]+anchor[3])/2)\n",
    "    cv2.circle(img, ptcxy, 1, (255,255,255), 2)\n",
    "    axes_array[iy,ix].imshow(img.astype(np.uint8))\n",
    "    axes_array[iy,ix].set_xlabel(\"Box no. {}\".format(ix+1))\n",
    "    axes_array[iy,ix].set_ylabel(\"Prior centre. {}\".format(iy+1))\n",
    "    \n",
    "plt.suptitle(\"Example of Anchor Boxes\", fontsize=20)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that here we use 6 anchor boxes of different shapes and scales for each prior centre of the feature map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bounding Box Prediction\n",
    "\n",
    "The model does not directly predict bounding boxes, but rather the probabilities and refinements which correspond to the anchor boxes mentioned above. This way, we can even detect overlapping objects. \n",
    "\n",
    "In practice, the extracted features are passed to the two-head predictor, a regressor and a classifier, yielding a per-anchor prediction. For each anchor, the regressor predicts a vector of localization shifting and stretching (cx, cy, w, h), while the classifier predicts a vector of per-class probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Initialize the predictor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metavision_ml.detection.box import deltas_to_bbox\n",
    "import torch.nn as nn\n",
    "\n",
    "# We build the predictor\n",
    "pd = BoxHead(rnn.cout, box_coder.num_anchors, num_classes + label_offset, nlayers)\n",
    "\n",
    "# let's jitter a bit the initialized localization prediction\n",
    "# in our code we initialize to 10x smaller standard deviation around the anchor box\n",
    "def initialize_layer(layer):\n",
    "    if isinstance(layer, nn.Conv2d):\n",
    "        nn.init.normal_(layer.weight, std=0.1)\n",
    "        if layer.bias is not None:\n",
    "            nn.init.constant_(layer.bias, val=0)\n",
    "                    \n",
    "pd.box_head.apply(initialize_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Get the predictions of bbox**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we get predictions for features\n",
    "deltas_loc_preds, cls_preds = pd(features)\n",
    "\n",
    "# we decode the bounding box predictions (without performing NMS and score filtering)\n",
    "box_preds = deltas_to_bbox(deltas_loc_preds, anchors_cxywh).data.numpy()[0].astype(np.int32)\n",
    "anchors = xywh2xyxy(anchors_cxywh).data.numpy()\n",
    "\n",
    "box_preds = box_preds[::70]\n",
    "anchors = anchors[::70]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Visualize some sampled anchors and their prediction**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img[...] = fill\n",
    "fig, axes_array= plt.subplots(nrows=len(anchors)//(box_coder.num_anchors), ncols=box_coder.num_anchors, sharex=True, sharey=True)\n",
    "\n",
    "\n",
    "for i in range(len(box_preds)):\n",
    "    img[...] = fill\n",
    "    \n",
    "    anchor = anchors[i]\n",
    "    pt1, pt2 = (anchor[0], anchor[1]), (anchor[2], anchor[3])\n",
    "    \n",
    "    pred = box_preds[i]\n",
    "    ppt1, ppt2 = (pred[0], pred[1]), (pred[2], pred[3])\n",
    "    \n",
    "    cv2.rectangle(img, pt1, pt2, (255,0,0), 1) # anchor box\n",
    "    cv2.rectangle(img, ppt1, ppt2, (0,0,255), 1) #prediction in blue\n",
    "    \n",
    "    ptcxy = int((anchor[0] + anchor[2])/2), int((anchor[1]+anchor[3])/2)\n",
    "    cv2.circle(img, ptcxy, 1, (255,255,255), 2)\n",
    "\n",
    "    ix, iy = i%box_coder.num_anchors, i//box_coder.num_anchors \n",
    "    axes_array[iy,ix].imshow(img.astype(np.uint8))\n",
    "    axes_array[iy,ix].set_xlabel(\"Box no. {}\".format(ix+1))\n",
    "    axes_array[iy,ix].set_ylabel(\"Prior centre. {}\".format(iy+1))\n",
    "\n",
    "plt.suptitle(\"Bounding Box prediction\", fontsize=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The predictions are shown in blue, and the anchors are shown in red. In the last image, we cannot see the anchor boxes because they are at the edge or outside of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMS\n",
    "\n",
    "Non-maximum suppression is a common strategy used in object detection. It helps prune redundant bounding boxes predicted for the same objects. Boxes with a low confidence score and IoU (Intersection over Union) less than a certain threshold are discarded. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Calculation\n",
    "\n",
    "Two loss functions are defined: \n",
    "\n",
    "- regression loss: e.g. focal loss\n",
    "- classification loss: e.g. smooth L1\n",
    "\n",
    "To better see what the loss vectors look like, let's run with the SSD model with moving-mnist dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from metavision_ml.detection.losses import DetectionLoss\n",
    "\n",
    "\n",
    "data = next(iter(dataloader))\n",
    "targets = data['labels']\n",
    "inputs = data['inputs']\n",
    "\n",
    "features = rnn(inputs)\n",
    "deltas_loc_preds, cls_preds = pd(features)\n",
    "\n",
    "targets = list(chain.from_iterable(targets))\n",
    "targets = box_coder.encode(features, x, targets) \n",
    "\n",
    "print('class target: ', targets['cls'].shape) # the class target: for each anchor, the assigned class\n",
    "print('localization target: ', targets['loc'].shape) # the localization target: for each anchor: the regression of difference w.r.t the anchor box\n",
    "print(targets.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = DetectionLoss(\"softmax_focal_loss\")\n",
    "loss = criterion(deltas_loc_preds, targets['loc'], cls_preds, targets[\"cls\"])\n",
    "\n",
    "print(loss.keys())\n",
    "print(loss['loc_loss'], loss['cls_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Pytorch-Lightning: Putting it together\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Let's now run a training loop with ``LightningDetectionModel`` of the class  :py:class:`metavision_ml.detection.lightning_model`. It is a module based on Pytorch-Lightning.\n",
    "\n",
    "We need to set hyperparameters defining:\n",
    "\n",
    "* neural network architecture\n",
    "* loss\n",
    "* dataset path\n",
    "* training schedule...\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ".. note::\n",
    "    Run the training pipeline with our :ref:`Python sample train_detection.py  <chapter_sdk_ml_train_detection>` for a better user experience.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ".. warning::\n",
    "    The training might be slow if you don't have a GPU."
   ]
  },  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "tmpdir = 'toy_problem_experiment'\n",
    "\n",
    "if os.path.exists(tmpdir):\n",
    "    shutil.rmtree(tmpdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params = argparse.Namespace(\n",
    "        in_channels= 3,  # number of input channels\n",
    "        feature_base=8,  # backbone width, number of channels doubles every CNN's octave\n",
    "        feature_channels_out=128,  # number of channels out for each rnn feature\n",
    "        anchor_list='MNIST_ANCHORS', # anchor configuration\n",
    "        loss='_focal_loss', #  type of classification loss\n",
    "        act='softmax', #  multinomial classification\n",
    "        feature_extractor='Vanilla',  # default architecture\n",
    "        classes=[str(i) for i in range(10)],  # MNIST classes names\n",
    "        dataset_path='toy_problem', # our dataset\n",
    "        lr=1e-3,  # learning rate\n",
    "        num_tbins=12, # number of time-steps per batch\n",
    "        batch_size=4,\n",
    "        height=128,\n",
    "        width=128,\n",
    "        max_frames_per_epoch=30000, # number of frames for one epoch\n",
    "        skip_us=0, \n",
    "        demo_every=2,  # launch a demonstration on video every 2 epochs\n",
    "        preprocess = 'none',  # type of preprocessing: in this case it is just RGB values, no event based data so far\n",
    "        max_boxes_per_input=500,  # max number of boxes wa can detect on a single frame  \n",
    "        delta_t=50000, # dummy duration per frame\n",
    "        root_dir=tmpdir, # logging directory\n",
    "        num_workers=4,  # number of data workers, ie processes loading the data\n",
    "        max_epochs=4,  # maximum number of epochs\n",
    "        precision=16,  # training with half precision will help consume less memory\n",
    "        lr_scheduler_step_gamma=0.98,  # learning_rate multiplication factor at each epoch\n",
    "    )\n",
    "\n",
    "model = LightningDetectionModel(params)\n",
    "\n",
    "tmpdir = os.path.join(params.root_dir, 'checkpoints')\n",
    "checkpoint_callback = ModelCheckpoint(dirpath=tmpdir, save_top_k=-1, period=1)\n",
    "logger = TestTubeLogger(\n",
    "    save_dir=os.path.join(params.root_dir, 'logs'),\n",
    "    version=1)\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    default_root_dir=params.root_dir, logger=logger,\n",
    "    checkpoint_callback=checkpoint_callback, gpus=1,\n",
    "    precision=params.precision, progress_bar_refresh_rate=1,\n",
    "    max_epochs=params.max_epochs,\n",
    "    check_val_every_n_epoch=1,\n",
    "    accumulate_grad_batches=1,\n",
    "    automatic_optimization=False,\n",
    "    log_every_n_steps=5)\n",
    "\n",
    "\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the TensorBoard, inspect the experiment directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run tensorboard in the background\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir toy_problem_experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Inference\n",
    "\n",
    "Our SSD model is trained! We can now run inference with some validation dataset..\n",
    "\n",
    "Here, we also provide a function to visualize the result, which is similar to the \"demo\" function in our Lightning Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "from metavision_ml.utils.main_tools import search_latest_checkpoint\n",
    "from metavision_ml.data import box_processing as box_api\n",
    "\n",
    "checkpoint_path = search_latest_checkpoint(\"toy_problem_experiment\") \n",
    "print('checkpoint path: ', checkpoint_path)\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "hparams = argparse.Namespace(**checkpoint['hyper_parameters'])\n",
    "\n",
    "model = LightningDetectionModel(hparams)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.eval()\n",
    "\n",
    "num_batches = 1\n",
    "batch_size = 4\n",
    "height, width = 128, 128\n",
    "tbins = 5\n",
    "dataloader = make_moving_mnist(train=False, height=height, width=width, tbins=tbins, num_workers=1,\n",
    "                               batch_size=batch_size, max_frames_per_video=10, max_frames_per_epoch=5000)\n",
    "label_map = ['background'] + [str(i) for i in range(10)]\n",
    "\n",
    "fig, axes_array = plt.subplots(nrows=batch_size, ncols=num_batches*tbins, figsize=(8,8))\n",
    "\n",
    "# this is more or less the code inside the function \"demo_video\":\n",
    "with torch.no_grad():\n",
    "    for batch_nb, batch in enumerate(islice(dataloader, num_batches)):\n",
    "        batch[\"inputs\"] = batch[\"inputs\"].to(model.device)\n",
    "        batch[\"mask_keep_memory\"] = batch[\"mask_keep_memory\"].to(model.device)\n",
    "        \n",
    "        images = batch[\"inputs\"].cpu().clone().data.numpy()\n",
    "        # inference\n",
    "        with torch.no_grad():\n",
    "            model.detector.reset(batch[\"mask_keep_memory\"])\n",
    "            predictions = model.detector.get_boxes(batch[\"inputs\"], score_thresh=0.3)\n",
    "        \n",
    "        # code to display the results\n",
    "        for t in range(len(images)):\n",
    "            for i in range(len(images[0])):\n",
    "                frame = dataloader.get_vis_func()(images[t][i])\n",
    "                pred = predictions[t][i]\n",
    "                target = batch[\"labels\"][t][i]\n",
    "\n",
    "                if isinstance(target, torch.Tensor):\n",
    "                    target = target.cpu().numpy()\n",
    "                if target.dtype.isbuiltin:\n",
    "                    target = box_api.box_vectors_to_bboxes(target[:, :4], target[:, 4])\n",
    "\n",
    "                if pred['boxes'] is not None:\n",
    "                    boxes = pred['boxes'].cpu().data.numpy()\n",
    "                    labels = pred['labels'].cpu().data.numpy()\n",
    "                    scores = pred['scores'].cpu().data.numpy()\n",
    "                    bboxes = box_api.box_vectors_to_bboxes(boxes, labels, scores)\n",
    "                    frame = draw_box_events(frame, bboxes, label_map, draw_score=True, thickness=2)\n",
    "\n",
    "                frame = draw_box_events(\n",
    "                    frame, target, label_map, force_color=[\n",
    "                        255, 255, 255], draw_score=False, thickness=1)\n",
    "\n",
    "                time = t + batch_nb * tbins\n",
    "                axes_array[i, t + batch_nb * tbins].imshow(frame.astype(np.uint8))\n",
    "                axes_array[i, t + batch_nb * tbins].set_xlabel(\"t=\"+str(time))\n",
    "                \n",
    "plt.suptitle(\"Inference result after training of 4 epochs\", fontsize=20)\n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ".. note::\n",
    "\n",
    "    Some precomputed datasets for automotive detection are listed :ref:`in the Datasets page <chapter_datasets_precomputed>` and are available for download.\n",
    "    \n",
    "    You can use them for a longer training on real event-based data.\n",
    "    \n",
    "    To run detection and tracking inference with our trained model, refer to :doc:`Detection and Tracking Tutorial <../inference/detection_and_tracking>`\n",
    "\n",
    "    This tutorial was created using `Jupiter Notebooks <https://jupyter.org/>`_\n",
    "\n",
    "    :download:`Download the tutorial code <object_detection_sequential_data.ipynb>`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
